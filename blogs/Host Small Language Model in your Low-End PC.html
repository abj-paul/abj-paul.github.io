<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Host Small Language Model in your Low-End PC</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="blogs.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body class="markdown-container">
<header id="title-block-header">
<h1 class="title">Host Small Language Model in your Low-End PC</h1>
<p class="date">&lt;2024-07-29 সোম&gt;</p>
</header>
<p>#+AUTHOR:</p>
<p>OuteAI published 300M and 65M parameter models, available in both instruct and base versions. 300M models have the Mistral architecture and 65M models have LLaMa architecture <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. So it made me wonder - can we use it as a simple chat assistant for low-end devices?</p>
<p>So I first built a simple chat application with it. The github link is: <a href="https://github.com/abj-paul/Small-AIChat">abj-paul/SmallAi-Chat</a>.</p>
<p><img src="../data/chat application.png" /></p>
<p>We then experimented with the responses. We soon noticed one problem. Language models return response equal to its context size. So usually, last part of the response is garbage. Some examples can be:</p>
<table>
<tbody>
<tr class="odd">
<td>Chat Example 1</td>
<td><a href="https://pastebin.com/TxHi09Tq">https://pastebin.com/TxHi09Tq</a></td>
</tr>
<tr class="even">
<td>Chat Example 2</td>
<td><a href="https://pastebin.com/SyLmBX5k">https://pastebin.com/SyLmBX5k</a></td>
</tr>
</tbody>
</table>
<p>So currently, I am looking at Post-Processing techniques and RAG.</p>
<h3 id="post-processing-language-model-outputs">Post-processing Language Model outputs</h3>
<p>We are looking for techniques to ensure that the response we send to user don't include garbage due to filling context length. We found the following techniques so far.</p>
<ol>
<li>Increasing temperature of model: This would ensure that the model gives more concise response. So we increased the temperature from 0.4 to 0.9 but it still gives response of full context length.</li>
<li>Truncate after 2-3 sentences: While it is true that for usual queries, the response should be small but it is not always the case. So this approach was not quite useful or intuitive.</li>
</ol>
<h3 id="rag-with-small-language-model">RAG with Small Language Model</h3>
<p>To understand RAG ability, we send the following as a prompt.</p>
<pre class="text"><code>Neil Armstrong was the first person to walk on the moon. Abhijit was the first to walk on failure. Now Who was the first person to walk on the moon?
</code></pre>
<p>The response was:</p>
<pre class="text"><code>The first person to walk on the moon was Neil Armstrong
</code></pre>
<p>We then experimented with one more realistic rag scenario.</p>
<pre class="text"><code>1Kg of potato costs 20 taka. 1Kg of guava costs 50taka. 1kg of rice costs 100taka. What is the cost of 1kg potato?
</code></pre>
<p>The response was…garbage. It seems its bad with RAG.</p>
<pre class="text"><code>To find the costs for potato, I can subtract the price from the cost. Since the price of a potato increases by an amount equal to 1 kg per taka, the cost of a product is $3^6 + 8 = 65$, or $7 + 9 = 49$. Using 1 million dollars and 100 taka, the product costs are $3 x (49 - 59) = 80x$, or $1,641,190 = $3 \times 75$. Therefore, the cost of 1kg of potato is $3 x 75 = 204 \times 75$. For the price of 1kg of potato, the total cost is $49 - 19 = 406$. Therefore, the price of a product is $3 x 25 = 260$.
</code></pre>
<h3 id="summary">Summary</h3>
<p>Small chat models return good responses when we want to learn something. For prompts like "Tell me about Large Language Model", "Tell me about the history of America", the responses were ok. They can also write poems or stories to a certain extent. They can be fun toy to play around when you don't have any internet connection.</p>
<h3 id="reference">Reference</h3>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://huggingface.co/OuteAI/Lite-Oute-1-300M-Instruct">https://huggingface.co/OuteAI/Lite-Oute-1-300M-Instruct</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
